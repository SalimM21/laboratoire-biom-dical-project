import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import altair as alt
import joblib # Pour charger le mod√®le et le scaler
from sklearn.preprocessing import StandardScaler # Pour le type hint et v√©rification

# --- Configuration de la page Streamlit ---
st.set_page_config(
    page_title="Syst√®me de Pr√©diction du Risque de Diab√®te",
    page_icon="ü©∫",
    layout="wide"
)

# --- Fonction de chargement des ressources (mod√®le et scaler) ---
@st.cache_resource
def load_resources():
    try:
        model = joblib.load('diabetes_risk_prediction_model.pkl')
        st.success("Mod√®le de pr√©diction charg√© avec succ√®s !", icon="‚úÖ")
    except FileNotFoundError:
        st.error("Erreur : Le fichier 'diabetes_risk_prediction_model.pkl' est introuvable. Assurez-vous qu'il est dans le m√™me r√©pertoire.", icon="‚ùå")
        st.stop()

    try:
        scaler = joblib.load('scaler.pkl')
        st.success("Scaler charg√© avec succ√®s !", icon="‚úÖ")
    except FileNotFoundError:
        st.error("Erreur : Le fichier 'scaler.pkl' est introuvable. Veuillez vous assurer que le scaler a √©t√© sauvegard√© depuis le notebook.", icon="‚ùå")
        st.stop()
    return model, scaler

# Charger le mod√®le et le scaler au d√©marrage de l'application
model, scaler = load_resources()

# --- Chargement et pr√©traitement des donn√©es (simul√©/simplifi√© pour la d√©mo) ---
# Dans une vraie application, vous chargeriez et appliqueriez ici le m√™me pr√©traitement que dans le notebook
@st.cache_data
def load_data():
    try:
        # Tente de charger votre dataset de diab√®te
        df = pd.read_csv('dataDiab√®te.csv')
        # Pour simuler la colonne 'Cluster' et 'risk_category' apr√®s clustering
        # C'est une SIMULATION, dans une vraie app, ces colonnes viendraient du vrai clustering
        if 'Cluster' not in df.columns:
            # Cr√©er des clusters al√©atoires pour la d√©mo si la colonne n'existe pas
            st.warning("La colonne 'Cluster' n'est pas pr√©sente dans 'dataDiab√®te.csv'. Des clusters al√©atoires seront g√©n√©r√©s pour la visualisation.")
            df['Cluster'] = np.random.randint(0, 2, size=len(df)) # 2 clusters: 0 ou 1
        if 'risk_category' not in df.columns:
            st.warning("La colonne 'risk_category' n'est pas pr√©sente. Elle sera g√©n√©r√©e √† partir des clusters pour la d√©mo.")
            df['risk_category'] = df['Cluster'].apply(lambda x: 'High Risk' if x == 1 else 'Low Risk')

        # Si vous voulez montrer le DF scaled, vous devriez le sauvegarder aussi du notebook
        # et le charger ici. Pour la simplicit√©, nous utilisons df original.
        return df
    except FileNotFoundError:
        st.error("Fichier 'dataDiab√®te.csv' non trouv√©. Veuillez le placer dans le m√™me r√©pertoire que l'application.", icon="‚ùå")
        st.stop()
    except Exception as e:
        st.error(f"Erreur lors du chargement des donn√©es : {e}", icon="‚ùå")
        st.stop()

df = load_data()
clustering_cols = ['Glucose', 'BMI', 'Age', 'DiabetesPedigreeFunction'] # D√©finition des colonnes pour les analyses


# --- Barre lat√©rale pour la navigation ---
st.sidebar.title("Navigation")
page = st.sidebar.radio("Aller √† la page :",
    ["Accueil",
     "1. Analyse Exploratoire des Donn√©es (EDA)",
     "2. Clustering (K-Means & PCA)",
     "3. Classification des Patients",
     "4. Pr√©diction du Risque"
    ]
)

# --- Contenu des pages ---

# Page d'Accueil
if page == "Accueil":
    st.image("https://via.placeholder.com/800x300.png?text=Image+d'accueil+du+projet", caption="Syst√®me intelligent de pr√©diction du risque de diab√®te")
    st.title("Syst√®me intelligent de pr√©diction du risque de diab√®te")
    st.markdown("""
    Ce projet vise √† d√©velopper un syst√®me intelligent capable de **pr√©dire le risque de d√©velopper le diab√®te** chez un patient et de **classifier les patients** en groupes √† risque.

    Utilisez le menu de navigation √† gauche pour explorer les diff√©rentes √©tapes de l'analyse,
    du pr√©traitement des donn√©es √† la pr√©diction finale du risque.
    """)
    st.info("Ce projet est une d√©monstration bas√©e sur un mod√®le de Machine Learning entra√Æn√© sur des donn√©es de sant√©.")


# Page 1: Analyse Exploratoire des Donn√©es (EDA)
elif page == "1. Analyse Exploratoire des Donn√©es (EDA)":
    st.header("1. Analyse Exploratoire des Donn√©es (EDA)")
    st.markdown("""
    Cette section pr√©sente un aper√ßu des donn√©es utilis√©es, leurs distributions et les relations entre les variables.
    """)

    st.subheader("Aper√ßu du Jeu de Donn√©es")
    st.write("Les 5 premi√®res lignes des donn√©es :")
    st.dataframe(df.head())
    st.write(f"**Dimensions du DataFrame :** {df.shape[0]} lignes, {df.shape[1]} colonnes.")
    st.write("Statistiques descriptives des variables num√©riques :")
    st.dataframe(df.describe())

    st.subheader("Distribution des Variables Num√©riques (Histogrammes)")
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    if numeric_cols:
        selected_eda_col = st.selectbox("S√©lectionnez une colonne √† visualiser :", numeric_cols, key="eda_hist_select")
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.histplot(df[selected_eda_col], kde=True, ax=ax)
        ax.set_title(f"Distribution de {selected_eda_col}")
        ax.set_xlabel(selected_eda_col)
        ax.set_ylabel("Fr√©quence")
        st.pyplot(fig)
    else:
        st.info("Aucune colonne num√©rique trouv√©e pour l'EDA.")

    st.subheader("Matrice de Corr√©lation")
    st.markdown("Visualisation des relations lin√©aires entre toutes les variables num√©riques.")
    fig, ax = plt.subplots(figsize=(10, 8))
    correlation_matrix = df.select_dtypes(include=np.number).corr()
    if not correlation_matrix.empty:
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", ax=ax)
        ax.set_title("Matrice de Corr√©lation")
        st.pyplot(fig)
    else:
        st.info("Aucune donn√©e num√©rique pour calculer la matrice de corr√©lation.")


# Page 2: Clustering (K-Means & PCA)
elif page == "2. Clustering (K-Means & PCA)":
    st.header("2. Clustering (K-Means & PCA)")
    st.markdown("""
    Cette section d√©montre comment les patients sont regroup√©s en clusters bas√©s sur leurs caract√©ristiques,
    et comment la PCA aide √† visualiser ces groupes.
    """)

    st.subheader("M√©thode du Coude pour K-Means")
    st.image("https://via.placeholder.com/600x400.png?text=Courbe+d'inertie+du+coude", caption="Exemple de courbe d'inertie (√† remplacer par votre propre graphique)")
    st.markdown("""
    La m√©thode du coude est utilis√©e pour d√©terminer le nombre optimal de clusters (k).
    Nous observons le point o√π la diminution de l'inertie commence √† ralentir significativement,
    indiquant la valeur de k optimale (souvent 2 ou 3 dans notre cas pour la cat√©gorie de risque).
    """)

    st.subheader("R√©partition des Observations par Cluster")
    # Simulation de la r√©partition si la colonne 'Cluster' est l√†
    if 'Cluster' in df.columns:
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.countplot(x='Cluster', data=df, ax=ax)
        ax.set_title('R√©partition des Observations par Cluster')
        ax.set_xlabel('Num√©ro de Cluster')
        ax.set_ylabel('Nombre d\'Observations')
        st.pyplot(fig)
        st.write("Comptage des observations par cluster :", df['Cluster'].value_counts().sort_index())
    else:
        st.warning("La colonne 'Cluster' n'est pas disponible pour la visualisation. Veuillez vous assurer que le clustering a √©t√© effectu√©.")


    st.subheader("Interpr√©tation des Clusters (Moyennes des Caract√©ristiques)")
    st.markdown("Les moyennes des caract√©ristiques pour chaque cluster nous aident √† comprendre leurs profils distincts.")
    if 'Cluster' in df.columns and all(col in df.columns for col in clustering_cols):
        # Utilisation de la version non-scaled du DF pour une meilleure interpr√©tation des moyennes
        cluster_means = df.groupby('Cluster')[clustering_cols].mean()
        st.dataframe(cluster_means)
        st.markdown(f"""
        Dans notre cas, si **Cluster 1** montre des valeurs moyennes plus √©lev√©es pour
        `Glucose`, `BMI` et `DiabetesPedigreeFunction` par rapport au **Cluster 0**,
        cela indique que le Cluster 1 correspond aux patients √† **risque √©lev√©** de diab√®te,
        tandis que le Cluster 0 correspond aux patients √† **faible risque**.
        """)
    else:
        st.info("Impossible de calculer les moyennes par cluster sans la colonne 'Cluster' ou les colonnes de clustering.")


    st.subheader("Visualisation des Clusters avec PCA (R√©duction de Dimensionnalit√©)")
    st.markdown("""
    L'Analyse en Composantes Principales (PCA) r√©duit la complexit√© des donn√©es en 2 dimensions
    pour faciliter la visualisation des clusters.
    """)
    # Pour cette partie, nous avons besoin de donn√©es scaled pour la PCA.
    # Dans une vraie app, vous feriez la PCA sur df_scaled.
    # Ici, nous allons simuler un df_pca pour la d√©mo si df n'est pas scaled
    try:
        from sklearn.decomposition import PCA
        # Re-scaling pour la d√©mo si df n'est pas pass√© par le pipeline complet
        temp_df_scaled = pd.DataFrame(scaler.fit_transform(df[clustering_cols]), columns=clustering_cols, index=df.index)
        pca = PCA(n_components=2)
        principal_components = pca.fit_transform(temp_df_scaled)
        df_pca = pd.DataFrame(data = principal_components, columns = ['PC1', 'PC2'])
        df_pca['Cluster'] = df['Cluster'] # Assurez-vous que la colonne Cluster est disponible

        chart_pca = alt.Chart(df_pca).mark_point(size=60).encode(
            x='PC1',
            y='PC2',
            color='Cluster:N', # 'N' pour Nominal (cat√©goriel)
            tooltip=['PC1', 'PC2', 'Cluster']
        ).properties(
            title="Clusters K-Means visualis√©s avec PCA"
        ).interactive() # Permet le zoom et le d√©placement
        st.altair_chart(chart_pca, use_container_width=True)
        st.write(f"Variance expliqu√©e par les 2 premi√®res composantes : {pca.explained_variance_ratio_.sum()*100:.2f}%")
    except Exception as e:
        st.error(f"Erreur lors de la visualisation PCA : {e}. Assurez-vous que les donn√©es et la colonne 'Cluster' sont correctement pr√©par√©es.", icon="‚ùå")


# Page 3: Classification des Patients
elif page == "3. Classification des Patients":
    st.header("3. Classification des Patients")
    st.markdown("""
    Cette section pr√©sente le processus de classification pour pr√©dire le risque de diab√®te.
    Nous avons entra√Æn√© et √©valu√© plusieurs mod√®les de Machine Learning.
    """)

    st.subheader("Pr√©paration des Donn√©es pour la Classification")
    st.markdown("""
    * **Variable Cible (Y) :** `risk_category` (Faible Risque / Haut Risque)
    * **Variables Explicatives (X) :** `Glucose`, `BMI`, `Age`, `DiabetesPedigreeFunction` (apr√®s mise √† l'√©chelle)
    * **Division des donn√©es :** 70% entra√Ænement, 30% test.
    * **Gestion du d√©s√©quilibre des classes :** Utilisation de sur-√©chantillonnage (`RandomOverSampler`) sur l'ensemble d'entra√Ænement pour garantir l'√©quit√© du mod√®le.
    """)
    st.write(f"**R√©partition des classes dans l'ensemble de donn√©es (apr√®s simulation si n√©cessaire) :**")
    st.write(df['risk_category'].value_counts())

    st.subheader("Mod√®les de Classification Test√©s")
    st.markdown("""
    Nous avons √©valu√© les algorithmes suivants :
    * **Random Forest Classifier**
    * **Support Vector Machine (SVM)**
    * **Gradient Boosting Classifier**
    * **R√©gression Logistique**
    """)

    st.subheader("√âvaluation des Mod√®les : M√©triques Cl√©s")
    st.markdown("Comparaison des performances des mod√®les sur l'ensemble de test, en privil√©giant le **F1-Score** pour un probl√®me avec d√©s√©quilibre de classes.")

    # Exemples de m√©triques (√Ä REMPLACER PAR VOS VRAIS R√âSULTATS DE NOTEBOOK)
    metrics_data = {
        'Mod√®le': ['Random Forest (Optimis√©)', 'Gradient Boosting (Optimis√©)', 'SVM', 'R√©gression Logistique'],
        'Accuracy': [0.855, 0.842, 0.798, 0.785],
        'Precision (High Risk)': [0.805, 0.795, 0.752, 0.740],
        'Recall (High Risk)': [0.838, 0.825, 0.725, 0.710],
        'F1-Score (High Risk)': [0.821, 0.809, 0.738, 0.725],
        'F1-Score CV Moyenne': [0.840, 0.830, 0.780, 0.770]
    }
    metrics_df = pd.DataFrame(metrics_data).set_index('Mod√®le')
    st.dataframe(metrics_df)

    st.subheader("Matrice de Confusion du Meilleur Mod√®le")
    st.markdown("La matrice de confusion du mod√®le `Random Forest (Optimis√©)` montre la pr√©cision des pr√©dictions :")
    conf_matrix_best_model = pd.DataFrame({
        'Pr√©dit Faible Risque': [170, 30],
        'Pr√©dit Haut Risque': [20, 130]
    }, index=['R√©el Faible Risque', 'R√©el Haut Risque'])
    st.dataframe(conf_matrix_best_model)
    st.markdown("""
    * **VP (Vrais Positifs) :** 130 patients √† haut risque correctement identifi√©s.
    * **VN (Vrais N√©gatifs) :** 170 patients √† faible risque correctement identifi√©s.
    * **FP (Faux Positifs) :** 20 patients √† faible risque class√©s par erreur comme √† haut risque.
    * **FN (Faux N√©gatifs) :** 30 patients √† haut risque class√©s par erreur comme √† faible risque (cas critique !).
    """)

    st.success("Le mod√®le **Random Forest (Optimis√©)** a √©t√© s√©lectionn√© comme le plus performant pour ce projet, offrant un bon √©quilibre entre pr√©cision et rappel pour la classe √† haut risque.")


# Page 4: Pr√©diction du Risque
elif page == "4. Pr√©diction du Risque":
    st.header("4. Pr√©diction du Risque de Diab√®te")
    st.markdown("""
    Utilisez ce formulaire interactif pour entrer les informations d'un nouveau patient
    et obtenir une pr√©diction imm√©diate de son risque de diab√®te.
    """)

    # --- Formulaire d'entr√©e pour la pr√©diction ---
    with st.form("prediction_form"):
        st.subheader("Informations du Patient :")
        col1, col2 = st.columns(2)
        glucose = col1.number_input('Glucose (mg/dL)', min_value=0, max_value=300, value=120, help="Niveau de glucose plasmatique √† 2 heures lors d'un test de tol√©rance au glucose.")
        bmi = col2.number_input('IMC (kg/m¬≤)', min_value=0.0, max_value=70.0, value=25.0, help="Indice de Masse Corporelle.")

        col3, col4 = st.columns(2)
        age = col3.number_input('√Çge', min_value=0, max_value=120, value=30, help="√Çge du patient en ann√©es.")
        diabetes_pedigree_function = col4.number_input('Fonction de Pr√©d. G√©n√©tique au Diab√®te', min_value=0.0, max_value=2.5, value=0.5, format="%.3f", help="Un score qui indique la probabilit√© de diab√®te bas√©e sur l'historique familial.")

        submitted = st.form_submit_button("Pr√©dire le Risque")

    if submitted:
        # Cr√©er un DataFrame avec les entr√©es utilisateur
        input_data = pd.DataFrame([[glucose, bmi, age, diabetes_pedigree_function]],
                                   columns=clustering_cols) # Utiliser clustering_cols pour s'assurer de l'ordre

        # Mettre √† l'√©chelle les donn√©es d'entr√©e en utilisant le scaler charg√©
        # Assurez-vous que 'scaler' est bien l'objet StandardScaler entra√Æn√©
        scaled_input_data = scaler.transform(input_data)

        # Effectuer la pr√©diction
        prediction = model.predict(scaled_input_data)

        st.subheader("R√©sultat de la Pr√©diction :")
        if prediction[0] == 'High Risk':
            st.error(f"Le patient est class√© comme **risque √©lev√©** de d√©velopper le diab√®te. Action recommand√©e : Consultation m√©dicale.", icon="üö®")
        else:
            st.success(f"Le patient est class√© comme **faible risque** de d√©velopper le diab√®te. Poursuite des bonnes habitudes de vie.", icon="üëç")

        st.info("Cette pr√©diction est un outil d'aide √† la d√©cision et ne remplace pas l'avis d'un professionnel de la sant√© qualifi√©.")